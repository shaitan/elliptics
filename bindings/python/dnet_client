#!/usr/bin/python

import errno
import heapq
import json
import itertools
import msgpack
import operator
import optparse
import os
import sys
import socket
import threading

from collections import OrderedDict
from collections import namedtuple

import elliptics


def print_result(data):
    """Pretty print result dict."""
    print json.dumps(data, indent=4)


def add_node_options(parser):
    """Common command line parser for all commands."""
    group = optparse.OptionGroup(parser, "Connection options",
                                 "Options for Elliptics client node. It is used to connect to server cluster")

    group.add_option("-g", "--groups", action="store", help="Group option.")
    group.add_option("-l", "--log", dest="log", default='/dev/stderr', metavar="FILE",
                     help="Output log messages from library to file [default: %default]")
    group.add_option("-L", "--log-level", action="store", type="int", dest="log_level",
                     default=elliptics.log_level.error,
                     help="Elliptics client verbosity [default: %default]")
    group.add_option("-r", "--remote", action="append", dest="remote", default=[],
                     help="Elliptics node address in format host:port:family [default: %default]")
    group.add_option("--wait-timeout", action="store", type="int", dest="wait_timeout", default=5,
                     help="Timeout for performing operations [default: %default]")
    group.add_option("--check-timeout", action="store", type="int", dest="check_timeout", default=30,
                     help="Timeout for route list requests [default: %default]")
    group.add_option('--port', action='store', type='int', default=1025,
                     help='Port which is listen by local dnet_ioserv [default: %default]. '
                          'If at least one remove is specified by -r/--remote this option will be ignored')
    group.add_option('-4', action='store_true', dest='ipv4', default=False,
                     help='Force to use IPv4 address for connecting to local elliptics node')
    group.add_option('-6', action='store_true', dest='ipv6', default=False,
                     help='Force to use IPv6 address for connecting to local elliptics node')
    group.add_option('-T', '--trace-id', dest='trace_id', default='0',
                     help='Mark all commands by trace_id at both local and server logs. '
                          'This option accepts hex strings. [default: %default]')

    parser.add_option_group(group)


def create_session(parser, options, single_remote=False, new_session=False, no_route_list=False):
    '''
    Method for initializing node, session and adding remotes by @options and @single_remote
    '''
    if not options.remote:
        family = 2 if options.ipv4 and not options.ipv6 else 10
        options.remote.append('{host}:{port}:{family}'.format(host=socket.gethostname(),
                                                              port=options.port,
                                                              family=family))
    if single_remote and len(options.remote) != 1:
        parser.error("Please specify exactly one remote address")
    elif not options.remote:
        parser.error("Please specify at least one remote address")

    groups = []

    try:
        if options.groups:
            groups = map(int, options.groups.split(','))
        else:
            groups = []
    except Exception as e:
        parser.error("Can't parse groups list: '{0}': {1}".format(options.groups, repr(e)))

    remotes = []

    try:
        for r in options.remote:
            remotes.append(str(elliptics.Address.from_host_port_family(r)))
    except Exception as e:
        parser.error("Can't parse host:port:family: '{0}': {1}".format(options.remote, repr(e)))

    cfg = elliptics.Config()
    cfg.wait_timeout = options.wait_timeout
    cfg.check_timeout = options.check_timeout
    cfg.nonblocking_io_thread_num = len(groups)
    cfg.net_thread_num = len(groups)
    cfg.io_thread_num = len(groups)

    if single_remote or no_route_list:
        cfg.flags |= elliptics.config_flags.no_route_list

    node = elliptics.create_node(remotes=remotes,
                                 log_level=options.log_level,
                                 log_file=options.log,
                                 cfg=cfg)

    session = elliptics.newapi.Session(node) if new_session else elliptics.Session(node)

    if len(groups) == 0:
        groups = session.routes.groups()

    session.groups = groups

    session.trace_id = int(options.trace_id, 16)

    return session


def print_backend_result(options, result):
    '''
    Pretty printer for backend status
    '''
    print_result({
        'backends': [
            {
                'backend_id': backend.backend_id,
                'state': backend.state,
                'defrag_state': int(backend.defrag_state),
                'last_start': str(backend.last_start),
                'last_start_err': backend.last_start_err,
                'readonly': backend.read_only,
                'inspect_state': int(backend.inspect_state),
            } for backend in result.get()[0].backends
            if options.backend_id == -1 or options.backend_id == backend.backend_id
        ]
    })


def process_backend_control(options, session, address, method):
    '''
    Common handler for backend commands called by:
    dnet_client backend enable ...
    or
    dnet_client backend disable ...
    etc.
    '''
    assert isinstance(session, elliptics.Session)

    result = method(session, address, options.backend_id)
    print_backend_result(options, result)


def process_backend_status(options, session, address):
    '''
    Handler for backend status command called by:
    dnet_client backend status -r `hostname -f`:1025:2
    '''
    assert isinstance(session, elliptics.Session)

    result = session.request_backends_status(address)
    print_backend_result(options, result)


def process_backend():
    '''
    Basic handler for all backend commands called by:
    dnet_client backend
    '''

    actions = {
        'enable': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.enable_backend),
        'disable': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.disable_backend),
        'remove': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.remove_backend),
        'defrag': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.start_defrag),
        'compact': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.start_compact),
        'stop_defrag': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.stop_defrag),
        'inspect': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.start_inspect),
        'stop_inspect': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.stop_inspect),
        'make_readonly': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.make_readonly),
        'make_writable': lambda x, y, z: process_backend_control(x, y, z, elliptics.Session.make_writable),
        'status': process_backend_status,
    }

    parser = optparse.OptionParser()
    parser.usage = '%prog backend action [options]'
    parser.add_option("--backend", dest="backend_id", type="int", default=-1,
                      help="Id of the backend where the command should be executed")

    add_node_options(parser)

    (options, args) = parser.parse_args()

    if len(args) > 2:
        parser.error("Too many arguments passed: {0}, expected: 2".format(len(args)))
    elif len(args) == 1 or args[1] not in actions:
        parser.error("Please specify one of following modes: {0}".format(', '.join(actions.keys())))

    action = args[1]

    if action != 'status' and options.backend_id < 0:
        parser.error("Please specify id of the backend")

    session = create_session(parser, options, single_remote=True)

    actions[action](options, session, elliptics.Address.from_host_port_family(options.remote[0]))


def process_monitor_control(options, session, method):
    '''
    Common handler for monitor commands called by:
    dnet_client monitor all     //for requesting monitor statistics from all cluster
    or
    dnet_client monitor one     // for requesting monitor statistics only from specified by -r/-remote node
    '''

    assert isinstance(session, elliptics.Session)
    categories = elliptics.monitor_stat_categories.all if options.categories == -1 else options.categories
    output = {}
    for result in method(session, categories=categories).get():
        output[str(result.address)] = result.statistics

    print json.dumps(output, sort_keys=True, indent=4, separators=(',', ': '))


def process_monitor():
    '''
    Basic handler for all monitor commands called by:
    dnet_client monitor
    '''
    from functools import partial

    parser = optparse.OptionParser()
    parser.usage = '%prog monitor action [options]'
    parser.add_option('--categories', dest='categories', type='int', default=-1,
                      help='Categories of monitor statistics that will be requested')
    parser.add_option('--backends', dest='backends', type='string',
                      help='Statistics will be requested for backends with specified ids'
                           '[e.g.: --backends=1,2,3]')
    add_node_options(parser)

    (options, args) = parser.parse_args()

    first_remote = elliptics.Address.from_host_port_family(options.remote[0])

    try:
        backends = None
        if options.backends:
            backends = set(map(int, options.backends.split(',')))
    except Exception as e:
        parser.error("Can't parse backends list: '{}': {}".format(options.backends, repr(e)))

    actions = {
        'all': lambda x, y: process_monitor_control(x, y, partial(elliptics.Session.monitor_stat, address=None,
                                                                  backends=backends)),
        'one': lambda x, y: process_monitor_control(x, y, partial(elliptics.Session.monitor_stat, address=first_remote,
                                                                  backends=backends))
    }

    if len(args) > 2:
        parser.error("Too many arguments passed: {0}, expected: 2".format(len(args)))
    elif len(args) == 1 or args[1] not in actions:
        parser.error("Please specify one of following modes: {0}".format(', '.join(actions.keys())))

    action = args[1]

    session = create_session(parser, options, single_remote=True)
    actions[action](options, session)


def process_iterator():
    """Handler for iterate commands called by: dnet_client iterate."""
    parser = optparse.OptionParser()
    parser.usage = '%prog iterate action [options]'
    parser.add_option('--json', action='store_true', dest='json', default=False,
                      help='request and print json of iterated keys')
    parser.add_option('--full', action='store_true', dest='full_print', default=False,
                      help='fully print iterated key')
    parser.add_option('--pretty', action='store_true', dest='pretty_print', default=False,
                      help='pretty print jsons and timestamps')
    parser.add_option('--backend', dest='backend_id', type='int',
                      help='id of the backend where keys should be iterated')
    parser.add_option('--user-flags', action='append', dest='user_flags_set', type='int',
                      default=[],
                      help='Filter keys which user_flags are specified by --user-flags.')
    add_node_options(parser)

    options, _ = parser.parse_args()

    session = create_session(parser, options, new_session=True, single_remote=True)

    flags = 0
    if options.json:
        flags |= elliptics.iterator_flags.json

    if options.backend_id is None:
        parser.error('Please specify backend (--backend=BACKEND_ID option) which should be iterated')

    if options.backend_id < 0:
        parser.error('Invalid value for backend option: "{}". It should be positive decimal'
                     .format(options.backend_id))

    first_remote = elliptics.Address.from_host_port_family(options.remote[0])

    async = session.start_iterator(address=first_remote,
                                   backend_id=options.backend_id,
                                   flags=flags,
                                   key_ranges=None,
                                   time_range=None)

    def dumps(obj):
        if options.pretty_print:
            return json.dumps(obj, indent=4, separators=(',', ': '))
        else:
            return json.dumps(obj)

    def dump_key(key):
        skey = str(key)
        return skey if options.full_print else '{}...{}'.format(skey[:12], skey[-12:])

    def dump_ts(ts):
        return str(ts) if options.pretty_print else ts.tsec

    for result in async:
        if options.user_flags_set and result.record_info.user_flags not in options.user_flags_set:
            # skip keys with user_flags which aren't specified by --user-flags
            continue
        print dumps(OrderedDict((
            ('key', dump_key(result.key)),
            ('attrs', OrderedDict((
                ('record_flags', elliptics.dump_record_flags(result.record_info.record_flags)),
                # ('user_flags', 0),
                ('json', OrderedDict((
                    ('ts', dump_ts(result.record_info.json_timestamp)),
                    ('size', result.record_info.json_size),
                    ('capacity', result.record_info.json_capacity),
                )) if result.record_info.json_capacity else None),
                ('data', OrderedDict((
                    ('ts', dump_ts(result.record_info.data_timestamp)),
                    ('size', result.record_info.data_size),
                )))
            ))),
            ('json', json.loads(result.json) if result.json else None),
        )))


def process_lookup():
    """Process dnet_client lookup."""
    parser = optparse.OptionParser()
    parser.usage = '%prog write [options]'

    parser.add_option('--key', dest='key', type='str',
                      help='Key by which write should be executed')
    parser.add_option('--hex', action='store_true', dest='hex', default=False,
                      help='Specifies that the key used with --key is hex string')

    add_node_options(parser)

    options, _ = parser.parse_args()

    key = elliptics.Id.from_hex(options.key) if options.hex else options.key

    session = create_session(parser, options, new_session=True, single_remote=False)

    session.exceptions_policy = elliptics.exceptions_policy.no_exceptions
    session.set_filter(elliptics.filters.all_final)

    results = []
    groups = session.groups
    for group in groups:
        session.groups = [int(group)]
        results.append(session.lookup(key))

    result = OrderedDict()
    for r in results:
        r = r.get()[0]
        group_result = OrderedDict(status=r.status)

        if r.path is not None:
            group_result['path'] = r.path

        if r.record_info is not None:
            group_result['record_info'] = OrderedDict((
                ('record_flags', elliptics.dump_record_flags(r.record_info.record_flags)),
                ('user_flags', r.record_info.user_flags),
                ('json_timestamp', str(r.record_info.json_timestamp)),
                ('json_offset', r.record_info.json_offset),
                ('json_size', r.record_info.json_size),
                ('json_capacity', r.record_info.json_capacity),
                ('data_timestamp', str(r.record_info.data_timestamp)),
                ('data_offset', r.record_info.data_offset),
                ('data_size', r.record_info.data_size),
            ))

        result[r.group_id] = group_result

    print_result(result)


def process_read():
    """Process dnet_client read."""
    parser = optparse.OptionParser()
    parser.usage = '%prog write [options]'
    parser.add_option('--key', dest='key', type='str',
                      help='Key by which write should be executed')
    parser.add_option('--hex', action='store_true', dest='hex', default=False,
                      help='Specifies that the key used with --key is hex string')
    parser.add_option('--data', action='store_true', dest='data', default=False,
                      help='Force dnet_client to read data')
    parser.add_option('--data-file', dest='data_file', metavar="FILE",
                      help='Force dnet_client to read data and write it to @FILE')
    parser.add_option('--data-offset', dest='data_offset', default=0, type='int',
                      help='Offset with which data should be read [default: %default]')
    parser.add_option('--data-size', dest='data_size', default=0, type='int',
                      help='Size of data part which should be read [default: %default]')
    parser.add_option('--json', action='store_true', dest='json', default=False,
                      help='Force dnet_client to read json')
    parser.add_option('--json-file', dest='json_file', metavar='FILE',
                      help='Force dnet_client to read json and write it to @FILE')

    add_node_options(parser)

    options, _ = parser.parse_args()

    if not options.key:
        parser.error('--key is not specified')

    if options.data and options.data_file:
        parser.error('options --data and --data-file are mutually exclusive')

    if options.json and options.json_file:
        parser.error('options --json and --json-file are mutually exclusive')

    key = elliptics.Id.from_hex(options.key) if options.hex else options.key

    session = create_session(parser, options, new_session=True, single_remote=False)

    read_data = options.data or options.data_file
    read_json = options.json or options.json_file

    # read both json and data if neither json or data is requested
    if not read_data and not read_json:
        read_data = read_json = True

    if read_json and read_data:
        async = session.read(key, options.data_offset, options.data_size)
    elif read_json:
        async = session.read_json(key)
    elif read_data:
        async = session.read_data(key, options.data_offset, options.data_size)
    else:
        parser.error('Unexpected combination of options')

    json_file = None
    if options.json_file:
        json_file = open(options.json_file, 'wb')

    data_file = None
    if options.data_file == options.json_file:
        data_file = json_file
    elif options.data_file:
        data_file = open(options.data_file, 'wb')

    r = async.get()[0]

    result = OrderedDict(group=r.group_id)

    if r.json:
        result['json_timestamp'] = str(r.record_info.json_timestamp)
        if json_file is not None:
            json_file.write(r.json)
            result['json'] = os.path.abspath(json_file.name)
            result['json_size'] = r.io_info.json_size
        else:
            result['json'] = json.loads(r.json)

    if r.data:
        result['data_timestamp'] = str(r.record_info.data_timestamp)
        if data_file is not None:
            data_file.write(r.data)
            result['data'] = os.path.abspath(data_file.name)
            result['data_size'] = r.io_info.data_size
        else:
            result['data'] = r.data

    if json_file:
        json_file.flush()
        json_file.close()

    if data_file and data_file != json_file:
        data_file.flush()
        data_file.close()

    print_result(result)


def process_write():
    """Process dnet_client write."""
    parser = optparse.OptionParser()
    parser.usage = '%prog write [options]'

    parser.add_option('--key', dest='key', type='str',
                      help='Key by which write should be executed')
    parser.add_option('--hex', action='store_true', dest='hex', default=False,
                      help='Specifies that the key used with --key is hex string')
    parser.add_option('--data', dest='data', type='str', default='',
                      help='Data which should be written into key (--key)')
    parser.add_option('--data-file', dest='data_file', metavar="FILE",
                      help='Get data from @FILE')
    parser.add_option('--data-capacity', dest='data_capacity', type='int', default=0,
                      help='Reserve @--data-capacity bytes for future data updates')
    parser.add_option('--json', dest='json', type='str', default='',
                      help='Json string which should be written into key (--key)')
    parser.add_option('--json-file', dest='json_file', metavar='FILE',
                      help='Get json from @FILE')
    parser.add_option('--json-capacity', dest='json_capacity', type='int', default=0,
                      help='Reserve @--json-capacity bytes for future json updates')

    add_node_options(parser)

    options, _ = parser.parse_args()

    if not options.key:
        parser.error('--key is not specified')

    if not options.data and not options.data_file:
        parser.error('--data or --data-file is not specified')
    elif options.data and options.data_file:
        parser.error('options --data and --data-file are mutually exclusive')

    if not options.json and not options.json_file:
        parser.error('--json or --json-file is not specified')
    elif options.json and options.json_file:
        parser.error('options --json and --json-file are mutually exclusive')

    key = elliptics.Id.from_hex(options.key) if options.hex else options.key

    session = create_session(parser, options, new_session=True, single_remote=False)

    session.set_checker(elliptics.checkers.no_check)
    session.set_filter(elliptics.filters.all_with_ack)

    json_string = options.json
    if options.json_file:
        with open(options.json_file, 'rb') as json_file:
            json_string = json_file.read()

    data = options.data
    if options.data_file:
        with open(options.data_file, 'rb') as data_file:
            data = data_file.read()

    async = session.write(key, json_string, options.json_capacity, data, options.data_capacity)

    result = OrderedDict()
    for r in async:
        group_result = OrderedDict(status=r.status)

        if r.path is not None:
            group_result['path'] = r.path

        if r.record_info is not None:
            group_result['record_info'] = OrderedDict((
                ('record_flags', elliptics.dump_record_flags(r.record_info.record_flags)),
                ('user_flags', r.record_info.user_flags),
                ('json_timestamp', str(r.record_info.json_timestamp)),
                ('json_offset', r.record_info.json_offset),
                ('json_size', r.record_info.json_size),
                ('json_capacity', r.record_info.json_capacity),
                ('data_timestamp', str(r.record_info.data_timestamp)),
                ('data_offset', r.record_info.data_offset),
                ('data_size', r.record_info.data_size),
            ))

        result[r.group_id] = group_result

    print_result(result)


def process_remove():
    """Process dnet_client remove."""
    parser = optparse.OptionParser()
    parser.usage = '%prog write [options]'

    parser.add_option('--key', dest='key', type='str',
                      help='Key by which write should be executed')
    parser.add_option('--hex', action='store_true', dest='hex', default=False,
                      help='Specifies that the key used with --key is hex string')

    add_node_options(parser)

    options, _ = parser.parse_args()

    if not options.key:
        parser.error('--key is not specified')

    key = elliptics.Id.from_hex(options.key) if options.hex else options.key

    session = create_session(parser, options, new_session=True, single_remote=False)

    session.set_checker(elliptics.checkers.no_check)
    session.set_filter(elliptics.filters.all_with_ack)

    async = session.remove(key)

    result = OrderedDict()
    for r in async:
        result[r.group_id] = OrderedDict(status=r.status)

    print_result(result)


def process_update_json():
    """Process dnet_client update_json."""
    parser = optparse.OptionParser()
    parser.usage = '%prog write [options]'

    parser.add_option('--key', dest='key', type='str',
                      help='Key by which write should be executed')
    parser.add_option('--hex', action='store_true', dest='hex', default=False,
                      help='Specifies that the key used with --key is hex string')
    parser.add_option('--json', dest='json', type='str',
                      help='Json string which should be written into key (--key)')
    parser.add_option('--json-file', dest='json_file', metavar='FILE',
                      help='Get json from @FILE')

    add_node_options(parser)

    options, _ = parser.parse_args()

    if not options.key:
        parser.error('--key is not specified')

    if not options.json and not options.json_file:
        parser.error('--json or --json-file is not specified')
    elif options.json and options.json_file:
        parser.error('options --json and --json-file are mutually exclusive')

    key = elliptics.Id.from_hex(options.key) if options.hex else options.key

    session = create_session(parser, options, new_session=True, single_remote=False)

    session.set_checker(elliptics.checkers.no_check)
    session.set_filter(elliptics.filters.all_with_ack)

    json_string = options.json
    if options.json_file:
        with open(options.json_file, 'rb') as json_file:
            json_string = json_file.read()

    async = session.update_json(key, json_string)

    result = OrderedDict()
    for r in async:
        group_result = OrderedDict(status=r.status)

        if r.path is not None:
            group_result['path'] = r.path

        if r.record_info is not None:
            group_result['record_info'] = OrderedDict((
                ('record_flags', elliptics.dump_record_flags(r.record_info.record_flags)),
                ('user_flags', r.record_info.user_flags),
                ('json_timestamp', str(r.record_info.json_timestamp)),
                ('json_offset', r.record_info.json_offset),
                ('json_size', r.record_info.json_size),
                ('json_capacity', r.record_info.json_capacity),
                ('data_timestamp', str(r.record_info.data_timestamp)),
                ('data_offset', r.record_info.data_offset),
                ('data_size', r.record_info.data_size),
            ))

        result[r.group_id] = group_result

    print_result(result)


# TODO: use extended KeyInfo when it will be possible
# KeyInfo = namedtuple('KeyInfo', ['key', 'group', 'status', 'json', 'json_ts', 'data_csum', 'data_ts'])
KeyInfo = namedtuple('KeyInfo', ['key', 'group', 'status', 'data_csum', 'data_ts'])


class Iterator(object):
    def __init__(self, session, group, ts, tmp_dir):
        self.exc_info = None

        self._ts = ts
        self._group = group
        self._session = session.clone()
        # use old session since new commands doesn't support getting checksums for json & data
        self._lookup_session = elliptics.Session(self._session._node)
        self._tmp_dir = tmp_dir
        self._final_dump = os.path.join(self._tmp_dir, '%s.keys') % self._group

        self._new_keys = set()  # keys written after the dnet_client start
        self._missed_keys = set()  # keys removed after iterator

        self._lookup_session.trace_id = self._session.trace_id
        self._session.groups = self._lookup_session.groups = [group]
        self._session.cflags = self._lookup_session.cflags = (elliptics.command_flags.nolock |
                                                              elliptics.command_flags.checksum)
        self._session.set_filter(elliptics.filters.all_with_ack)
        self._lookup_session.set_filter(elliptics.filters.all_with_ack)
        self._lookup_session.exceptions_policy = elliptics.exceptions_policy.no_exceptions

    def prepare(self):
        try:
            self._iterate()
            self._read()
            self._merge()
        except Exception:
            self.exc_info = sys.exc_info()

    def _iterate(self):
        """Iterate backend's keys and split them by blob."""
        backends = self._session.routes.filter_by_group(self._group).addresses_with_backends()
        if len(backends) != 1:
            raise RuntimeError('Route-list for group: {} contains several backends: {}'.format(self._group, backends))

        address, backend_id = backends[0]

        self._session.set_direct_id(address=address, backend_id=backend_id)

        async = self._session.start_iterator(address=address,
                                             backend_id=backend_id,
                                             flags=elliptics.iterator_flags.no_meta,
                                             key_ranges=None,
                                             time_range=None)

        dumps = {}
        for result in async:
            if result.is_ack:
                # skip ack package without data
                continue
            dump = dumps.get(result.blob_id, None)
            if dump is None:
                dump = open(os.path.join(self._tmp_dir, '%s-%s.keys') % (self._group, result.blob_id), 'wb')
                dumps[result.blob_id] = dump
            msgpack.pack((str(result.key), result.record_info.data_offset), dump)

        self._dumps = [dump.name for dump in dumps.values()]
        for dump in dumps.values():
            dump.close()

    def _read(self):
        dumps_count = len(self._dumps)
        for dump_idx, dump_path in enumerate(self._dumps, start=1):
            print 'reading keys: {} ({}/{})\n'.format(dump_path, dump_idx, dumps_count),
            key_infos = []
            with open(dump_path, 'rb') as dump:
                for _, key in sorted(((offset, key) for key, offset in msgpack.Unpacker(dump))):
                    result = self._lookup_session.lookup(elliptics.Id(key, self._group)).get()[0]
                    if result.status == -errno.ENOENT:
                        self._missed_keys.add(key)

                    if result.status < 0:
                        key_info = KeyInfo(key=key,
                                           group=self._group,
                                           status=result.status,
                                           data_csum=None,
                                           data_ts=None)
                    elif result.status > 0:
                        print '{} key lookup has unexpected status: {}. Is it ping?\n'.format(key, result.status)
                        continue
                    elif result.timestamp > self._ts:
                        key_info = KeyInfo(key=key,
                                           group=self._group,
                                           status=1,  # the key was overwritten
                                           data_csum=str(result.checksum),
                                           data_ts=(result.timestamp.tsec,
                                                    result.timestamp.tnsec))
                        self._new_keys.add(key)
                    else:
                        key_info = KeyInfo(key=key,
                                           group=self._group,
                                           status=1,
                                           data_csum=str(result.checksum),
                                           data_ts=(result.timestamp.tsec,
                                                    result.timestamp.tnsec))
                    key_infos.append(key_info)
            # TODO: use bulk_read when it will be possible to get checksum for json and data in such way
            # fetch from dump all keys sorted by offset
            # with open(dump_path, 'rb') as dump:
            #     keys = [elliptics.Id(k, self._group)
            #             for _, k in sorted(((offset, key) for key, offset in msgpack.Unpacker(dump)))]
            #
            # key_infos = []
            # for result in self._session.bulk_read_json(keys):
            #     if result.status < 0:
            #         key_info = KeyInfo(key=str(result.id),
            #                            group=self._group,
            #                            status=result.status,
            #                            json=None,
            #                            json_ts=None,
            #                            data_csum=None,
            #                            data_ts=None)
            #         if result.status == -errno.ENOENT:
            #             self._missed_keys.add(str(result.id))
            #     elif result.status > 0:
            #         print 'some positive status(', result.status, '), maybe ping?'
            #         continue
            #     elif result.record_info.data_timestamp > self._ts or result.record_info.json_timestamp > self._ts:
            #         key_info = KeyInfo(key=str(result.id),
            #                            group=self._group,
            #                            status=1,  # the key was overwritten
            #                            json=result.json,
            #                            json_ts=(result.record_info.json_timestamp.tsec,
            #                                     result.record_info.json_timestamp.tnsec),
            #                            data_csum=None,
            #                            data_ts=(result.record_info.data_timestamp.tsec,
            #                                     result.record_info.data_timestamp.tnsec))
            #         self._new_keys.add(str(result.id))
            #     else:
            #         key_info = KeyInfo(key=str(result.id),
            #                            group=self._group,
            #                            status=1,
            #                            json=result.json,
            #                            json_ts=(result.record_info.json_timestamp.tsec,
            #                                     result.record_info.json_timestamp.tnsec),
            #                            data_csum=None,
            #                            data_ts=(result.record_info.data_timestamp.tsec,
            #                                     result.record_info.data_timestamp.tnsec))
            #     key_infos.append(key_info)

            key_infos.sort(key=operator.itemgetter(0))  # sort by key_info.key
            with open(dump_path, 'wb') as dump:
                for key_info in key_infos:
                    msgpack.pack(key_info, dump)

            print '{} keys were read from {}\n'.format(len(key_infos), dump_path)
        print '{} keys were written after start\n'.format(len(self._new_keys))
        print '{} keys were removed after iterating\n'.format(len(self._missed_keys))

    def _merge(self):
        heap = []

        def heap_push(dump_file, unpacker=None):
            try:
                unpacker = unpacker or msgpack.Unpacker(dump_file)
                heapq.heappush(heap, (next(unpacker), (dump_file, unpacker)))
            except StopIteration:
                dump_file.close()
                os.remove(dump_file.name)

        def heap_pop():
            value, it = heapq.heappop(heap)
            heap_push(*it)
            return KeyInfo(*value)

        prev = None
        with open(self._final_dump, 'wb') as final_dump:
            for dump_path in self._dumps:
                heap_push(dump_file=open(dump_path, 'rb'))

            while heap:
                key_info = heap_pop()
                if prev != key_info.key:
                    # pack only unique keys
                    msgpack.pack(key_info, final_dump)
                else:
                    print prev, ' is non-unique in group: ', self._group
                prev = key_info.key

    def __iter__(self):
        with open(self._final_dump, 'rb') as final_dump:
            for key_info in msgpack.Unpacker(final_dump):
                yield KeyInfo(*key_info)
        os.remove(self._final_dump)


def analyze(groups, key_infos):
    assert all(key_info.key == key_infos[0].key for key_info in key_infos)

    report = ''
    correct_infos = [key_info for key_info in key_infos if key_info.status >= 0]

    missed = set(groups) - {key_info.group for key_info in key_infos}
    if missed:
        report += '\tmissed in groups: %s' % missed

    corrupted = {key_info.groups: key_info.status
                 for key_info in key_infos
                 if key_info.status in {-errno.EILSEQ, -errno.EINVAL, -errno.ERANGE}}
    if corrupted:
        report += '\tcorrupted in groups: %s' % corrupted

    # jsons, datas, json_tss, data_tss = {}, {}, {}, {}
    datas, data_tss = {}, {}
    for key_info in correct_infos:
        if key_info.data_csum not in datas:
            datas[key_info.data_csum] = [key_info.group]
        else:
            datas[key_info.data_csum].append(key_info.group)

        data_ts = str(elliptics.Time(key_info.data_ts[0], key_info.data_ts[1]))
        if data_ts not in data_tss:
            data_tss[data_ts] = [key_info.group]
        else:
            data_tss[data_ts].append(key_info.group)

        # json_hash = hashlib.sha512(key_info.json).hexdigest()
        # if json_hash not in jsons:
        #     jsons[json_hash] = [key_info.group]
        # else:
        #     jsons[json_hash].append(key_info.group)
        #
        # json_ts = str(elliptics.Time(key_info.json_ts[0], key_info.json_ts[1]))
        # if json_ts not in json_tss:
        #     json_tss[json_ts] = [key_info.group]
        # else:
        #     json_tss[json_ts].append(key_info.group)

    if len(datas) > 1:
        report += '\tdata mismatch: %s' % {hash[:12]: groups for hash, groups in datas.iteritems()}

    if len(data_tss) > 1:
        report += '\tdata timestamp mismatch: %s' % {ts: groups for ts, groups in data_tss.iteritems()}

    # if len(jsons) > 1:
    #     report += '\tjson mismatch: %s' % {hash[:12]: groups for hash, groups in jsons.iteritems()}
    #
    # if len(json_tss) > 1:
    #     report += '\tjson timestamp mismatch: %s' % {str(elliptics.Time(ts[0], ts[1])): groups
    #                                                  for ts, groups in json_tss.iteritems()}

    statuses = {}
    for key_info in key_infos:
        if key_info.status not in statuses:
            statuses[key_info.status] = [key_info.group]
        else:
            statuses[key_info.status].append(key_info.group)

    if len(statuses) > 1:
        report += '\tstatus mismatch: %s' % {status: groups for status, groups in statuses.iteritems()}

    if report != '':
        return key_infos[0].key + report + '\n'
    else:
        return ''


def process_compare():
    """Process dnet_client compare."""
    parser = optparse.OptionParser()
    parser.usage = '%prog compare [options]'
    parser.add_option('--tmp', dest='tmp_dir', type='str', default=None,
                      help='Path to temporary directory where temporary files and found inconsistent'
                           'keys should be written.')

    add_node_options(parser)

    options = parser.parse_args()[0]

    if options.groups is None:
        parser.error('-g/--groups is not specified')

    groups = set(map(int, options.groups.split(',')))

    if options.tmp_dir is None:
        parser.error('--tmp is not specified')

    if not os.path.exists(options.tmp_dir):
        os.makedirs(options.tmp_dir)

    session = create_session(parser, options, new_session=True, no_route_list=True)
    session.set_checker(elliptics.checkers.no_check)

    if not groups <= set(session.routes.groups()):
        parser.error('specified groups ({}) were not found in route-list'
                     .format(list(groups - set(session.routes.groups()))))

    ts = elliptics.Time.now()

    iterators = tuple(Iterator(session, group, ts, options.tmp_dir) for group in groups)
    threads = tuple(threading.Thread(target=iterator.prepare) for iterator in iterators)

    for thread in threads:
        thread.daemon = True
        thread.start()

    alive = True
    while alive:
        alive = False
        for thread, iterator in itertools.izip(threads, iterators):
            thread.join(timeout=1)
            if thread.isAlive():
                alive = True
            elif iterator.exc_info:
                raise iterator.exc_info[0], iterator.exc_info[1], iterator.exc_info[2]

    del threads

    heap = []

    def heap_push(iterator):
        try:
            heapq.heappush(heap, (next(iterator), iterator))
        except StopIteration:
            pass

    def heap_pop():
        value, iterator = heapq.heappop(heap)
        heap_push(iterator)
        return value

    for iterator in iterators:
        heap_push(iter(iterator))

    with open(os.path.join(options.tmp_dir, 'report'), 'w') as report_file:
        key_infos = []
        while heap:
            key_info = heap_pop()
            if not key_infos or key_infos[0].key == key_info.key:
                key_infos.append(key_info)
                continue
            report_file.write(analyze(groups, key_infos))
            key_infos = [key_info]


def main():
    commands = {
        'backend': process_backend,
        'monitor': process_monitor,
        'iterate': process_iterator,
        'lookup': process_lookup,
        'read': process_read,
        'write': process_write,
        'remove': process_remove,
        'update_json': process_update_json,
        'compare': process_compare
    }

    parser = optparse.OptionParser()
    parser.usage = '%prog command [options]'
    parser.description = 'dnet_client is client for Elliptics distributed key-value storage'
    parser.epilog = 'Supported commands are: {0}'.format(', '.join(commands.keys()))

    _, args = parser.parse_args(args=sys.argv[1:2])

    if len(args) > 1:
        parser.error("Too many arguments passed: {0}, expected: 1".format(len(args)))
    elif len(args) == 0 or args[0] not in commands:
        parser.error("Please specify one of following modes: {0}".format(', '.join(commands.keys())))

    commands[args[0]]()


if __name__ == '__main__':
    try:
        main()
    except elliptics.Error as e:
        print_result({
            'error': {
                'code': e.message.code,
                'message': str(e)
            }
        })
        sys.exit(1)
